{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Huggingface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12de22cb6f664e459cfad5b80b5bbffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icapr\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\icapr\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f075555f53a4baba697f253a2d15689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading a model (e.g. )\n",
    "from transformers import AutoModel\n",
    "encoder = AutoModel.from_pretrained(\"xlm-roberta-base\", add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.LayerNorm.weight',\n",
       " 'embeddings.LayerNorm.bias',\n",
       " 'encoder.layer.0.attention.self.query.weight',\n",
       " 'encoder.layer.0.attention.self.query.bias',\n",
       " 'encoder.layer.0.attention.self.key.weight',\n",
       " 'encoder.layer.0.attention.self.key.bias',\n",
       " 'encoder.layer.0.attention.self.value.weight',\n",
       " 'encoder.layer.0.attention.self.value.bias',\n",
       " 'encoder.layer.0.attention.output.dense.weight',\n",
       " 'encoder.layer.0.attention.output.dense.bias',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.0.intermediate.dense.weight',\n",
       " 'encoder.layer.0.intermediate.dense.bias',\n",
       " 'encoder.layer.0.output.dense.weight',\n",
       " 'encoder.layer.0.output.dense.bias',\n",
       " 'encoder.layer.0.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.attention.self.query.weight',\n",
       " 'encoder.layer.1.attention.self.query.bias',\n",
       " 'encoder.layer.1.attention.self.key.weight',\n",
       " 'encoder.layer.1.attention.self.key.bias',\n",
       " 'encoder.layer.1.attention.self.value.weight',\n",
       " 'encoder.layer.1.attention.self.value.bias',\n",
       " 'encoder.layer.1.attention.output.dense.weight',\n",
       " 'encoder.layer.1.attention.output.dense.bias',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.intermediate.dense.weight',\n",
       " 'encoder.layer.1.intermediate.dense.bias',\n",
       " 'encoder.layer.1.output.dense.weight',\n",
       " 'encoder.layer.1.output.dense.bias',\n",
       " 'encoder.layer.1.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.attention.self.query.weight',\n",
       " 'encoder.layer.2.attention.self.query.bias',\n",
       " 'encoder.layer.2.attention.self.key.weight',\n",
       " 'encoder.layer.2.attention.self.key.bias',\n",
       " 'encoder.layer.2.attention.self.value.weight',\n",
       " 'encoder.layer.2.attention.self.value.bias',\n",
       " 'encoder.layer.2.attention.output.dense.weight',\n",
       " 'encoder.layer.2.attention.output.dense.bias',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.intermediate.dense.weight',\n",
       " 'encoder.layer.2.intermediate.dense.bias',\n",
       " 'encoder.layer.2.output.dense.weight',\n",
       " 'encoder.layer.2.output.dense.bias',\n",
       " 'encoder.layer.2.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.attention.self.query.weight',\n",
       " 'encoder.layer.3.attention.self.query.bias',\n",
       " 'encoder.layer.3.attention.self.key.weight',\n",
       " 'encoder.layer.3.attention.self.key.bias',\n",
       " 'encoder.layer.3.attention.self.value.weight',\n",
       " 'encoder.layer.3.attention.self.value.bias',\n",
       " 'encoder.layer.3.attention.output.dense.weight',\n",
       " 'encoder.layer.3.attention.output.dense.bias',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.intermediate.dense.weight',\n",
       " 'encoder.layer.3.intermediate.dense.bias',\n",
       " 'encoder.layer.3.output.dense.weight',\n",
       " 'encoder.layer.3.output.dense.bias',\n",
       " 'encoder.layer.3.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.attention.self.query.weight',\n",
       " 'encoder.layer.4.attention.self.query.bias',\n",
       " 'encoder.layer.4.attention.self.key.weight',\n",
       " 'encoder.layer.4.attention.self.key.bias',\n",
       " 'encoder.layer.4.attention.self.value.weight',\n",
       " 'encoder.layer.4.attention.self.value.bias',\n",
       " 'encoder.layer.4.attention.output.dense.weight',\n",
       " 'encoder.layer.4.attention.output.dense.bias',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.intermediate.dense.weight',\n",
       " 'encoder.layer.4.intermediate.dense.bias',\n",
       " 'encoder.layer.4.output.dense.weight',\n",
       " 'encoder.layer.4.output.dense.bias',\n",
       " 'encoder.layer.4.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.attention.self.query.weight',\n",
       " 'encoder.layer.5.attention.self.query.bias',\n",
       " 'encoder.layer.5.attention.self.key.weight',\n",
       " 'encoder.layer.5.attention.self.key.bias',\n",
       " 'encoder.layer.5.attention.self.value.weight',\n",
       " 'encoder.layer.5.attention.self.value.bias',\n",
       " 'encoder.layer.5.attention.output.dense.weight',\n",
       " 'encoder.layer.5.attention.output.dense.bias',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.intermediate.dense.weight',\n",
       " 'encoder.layer.5.intermediate.dense.bias',\n",
       " 'encoder.layer.5.output.dense.weight',\n",
       " 'encoder.layer.5.output.dense.bias',\n",
       " 'encoder.layer.5.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.attention.self.query.weight',\n",
       " 'encoder.layer.6.attention.self.query.bias',\n",
       " 'encoder.layer.6.attention.self.key.weight',\n",
       " 'encoder.layer.6.attention.self.key.bias',\n",
       " 'encoder.layer.6.attention.self.value.weight',\n",
       " 'encoder.layer.6.attention.self.value.bias',\n",
       " 'encoder.layer.6.attention.output.dense.weight',\n",
       " 'encoder.layer.6.attention.output.dense.bias',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.intermediate.dense.weight',\n",
       " 'encoder.layer.6.intermediate.dense.bias',\n",
       " 'encoder.layer.6.output.dense.weight',\n",
       " 'encoder.layer.6.output.dense.bias',\n",
       " 'encoder.layer.6.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.attention.self.query.weight',\n",
       " 'encoder.layer.7.attention.self.query.bias',\n",
       " 'encoder.layer.7.attention.self.key.weight',\n",
       " 'encoder.layer.7.attention.self.key.bias',\n",
       " 'encoder.layer.7.attention.self.value.weight',\n",
       " 'encoder.layer.7.attention.self.value.bias',\n",
       " 'encoder.layer.7.attention.output.dense.weight',\n",
       " 'encoder.layer.7.attention.output.dense.bias',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.intermediate.dense.weight',\n",
       " 'encoder.layer.7.intermediate.dense.bias',\n",
       " 'encoder.layer.7.output.dense.weight',\n",
       " 'encoder.layer.7.output.dense.bias',\n",
       " 'encoder.layer.7.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.attention.self.query.weight',\n",
       " 'encoder.layer.8.attention.self.query.bias',\n",
       " 'encoder.layer.8.attention.self.key.weight',\n",
       " 'encoder.layer.8.attention.self.key.bias',\n",
       " 'encoder.layer.8.attention.self.value.weight',\n",
       " 'encoder.layer.8.attention.self.value.bias',\n",
       " 'encoder.layer.8.attention.output.dense.weight',\n",
       " 'encoder.layer.8.attention.output.dense.bias',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.intermediate.dense.weight',\n",
       " 'encoder.layer.8.intermediate.dense.bias',\n",
       " 'encoder.layer.8.output.dense.weight',\n",
       " 'encoder.layer.8.output.dense.bias',\n",
       " 'encoder.layer.8.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.attention.self.query.weight',\n",
       " 'encoder.layer.9.attention.self.query.bias',\n",
       " 'encoder.layer.9.attention.self.key.weight',\n",
       " 'encoder.layer.9.attention.self.key.bias',\n",
       " 'encoder.layer.9.attention.self.value.weight',\n",
       " 'encoder.layer.9.attention.self.value.bias',\n",
       " 'encoder.layer.9.attention.output.dense.weight',\n",
       " 'encoder.layer.9.attention.output.dense.bias',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.intermediate.dense.weight',\n",
       " 'encoder.layer.9.intermediate.dense.bias',\n",
       " 'encoder.layer.9.output.dense.weight',\n",
       " 'encoder.layer.9.output.dense.bias',\n",
       " 'encoder.layer.9.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.attention.self.query.weight',\n",
       " 'encoder.layer.10.attention.self.query.bias',\n",
       " 'encoder.layer.10.attention.self.key.weight',\n",
       " 'encoder.layer.10.attention.self.key.bias',\n",
       " 'encoder.layer.10.attention.self.value.weight',\n",
       " 'encoder.layer.10.attention.self.value.bias',\n",
       " 'encoder.layer.10.attention.output.dense.weight',\n",
       " 'encoder.layer.10.attention.output.dense.bias',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.intermediate.dense.weight',\n",
       " 'encoder.layer.10.intermediate.dense.bias',\n",
       " 'encoder.layer.10.output.dense.weight',\n",
       " 'encoder.layer.10.output.dense.bias',\n",
       " 'encoder.layer.10.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.attention.self.query.weight',\n",
       " 'encoder.layer.11.attention.self.query.bias',\n",
       " 'encoder.layer.11.attention.self.key.weight',\n",
       " 'encoder.layer.11.attention.self.key.bias',\n",
       " 'encoder.layer.11.attention.self.value.weight',\n",
       " 'encoder.layer.11.attention.self.value.bias',\n",
       " 'encoder.layer.11.attention.output.dense.weight',\n",
       " 'encoder.layer.11.attention.output.dense.bias',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.intermediate.dense.weight',\n",
       " 'encoder.layer.11.intermediate.dense.bias',\n",
       " 'encoder.layer.11.output.dense.weight',\n",
       " 'encoder.layer.11.output.dense.bias',\n",
       " 'encoder.layer.11.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.output.LayerNorm.bias']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, _ in encoder.named_parameters()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data (*small* map-style datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7067ab846b3421aab0b792c2ee81ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icapr\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\icapr\\.cache\\huggingface\\hub\\datasets--stsb_multi_mt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670ebac3bfa34ae9af1b26a6553b172f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/470k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa11a4ff1e9401f838d4b3d0f855c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df73234404d04d2c86df4b0d025c27fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev-00000-of-00001.parquet:   0%|          | 0.00/142k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544648484f744c83849032ce030874e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c0cda9791e4782b4eb63bb41ca40b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b042c6603d4342f98505ff195a67bed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading datasets\n",
    "from datasets import load_dataset\n",
    "stsb = load_dataset(path=\"stsb_multi_mt\", name=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073941f840c34d99b7faab13eb23619c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51da7e83c8147bc9bb60d4081aee311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e98f31e76b642fc8ad4515fb70a6799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess a dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,    62, 47880,    83, 35971,  5773,     5,     2,     2,   893,\n",
      "          1831, 47880,    83, 35971,  5773,     5,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "train_examples = tokenizer(stsb[\"train\"][\"sentence1\"][0], stsb[\"train\"][\"sentence2\"][0], truncation=True, padding=\"max_length\", max_length=32, return_tensors='pt')\n",
    "print(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2114,  0.2636,  0.1051,  ..., -0.2435,  0.2229, -0.1088],\n",
      "         [-0.0390,  0.0091,  0.0044,  ..., -0.0226,  0.0616,  0.2126],\n",
      "         [-0.0664,  0.1254, -0.0792,  ...,  0.1323,  0.0104, -0.0032],\n",
      "         ...,\n",
      "         [ 0.2524,  0.3314, -0.1276,  ..., -0.7320, -0.0146,  0.0453],\n",
      "         [ 0.2524,  0.3314, -0.1276,  ..., -0.7320, -0.0146,  0.0453],\n",
      "         [ 0.2524,  0.3314, -0.1276,  ..., -0.7320, -0.0146,  0.0453]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = encoder(input_ids=train_examples[\"input_ids\"], attention_mask=train_examples[\"attention_mask\"])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output representations\n",
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooled CLS token\n",
    "# output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we want to pad to max length all the time?\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d5f6a695f1480e8c879436b91ae3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4f034c144647bfa4d1f99187954275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8aa7871c86443ad9acdaca5e3848cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = stsb.map(tokenize_function, batched=True, remove_columns=[\"sentence1\", \"sentence2\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['similarity_score', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['similarity_score', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['similarity_score', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format of all columns to torch tensors\n",
    "tokenized_datasets[\"train\"].set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# No need to define a torch.utils.data.Dataset\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape\n",
    "\n",
    "# ==> Proceed for val and test in similar fashion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=False, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d08faa42244de4854e59cfa5dc447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78847fd15a62448f9d87ca10bde7da70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f59e0df4e145b9832cb4a183abdf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = stsb.map(tokenize_function, batched=True, remove_columns=[\"sentence1\", \"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarity_score': 5.0,\n",
       " 'input_ids': [0,\n",
       "  62,\n",
       "  47880,\n",
       "  83,\n",
       "  35971,\n",
       "  5773,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  893,\n",
       "  1831,\n",
       "  47880,\n",
       "  83,\n",
       "  35971,\n",
       "  5773,\n",
       "  5,\n",
       "  2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to define a torch.utils.data.Dataset\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 47])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72768256ce09aebce0ac4dd5209fef3a694ce4cdd1a70ecd6912bedba09dd60c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
